{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "resident-special",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6ecb168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk import bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-kinase",
   "metadata": {},
   "source": [
    "### import the data and store it as an array in the variable: corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42ae77a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('blogtext_5k.csv')\n",
    "corpus = data['text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80befaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(corpus):\n",
    "    # loop through each document in the corpus\n",
    "    for i in range(len(corpus)):\n",
    "        # remove extra whitespace and then lowercase all letters\n",
    "        temp = re.sub('[^a-z ]','',corpus[i].strip().lower())\n",
    "\n",
    "        # split and rejoin document to remove extra space within text\n",
    "        temp = [word for word in temp.split()]\n",
    "        temp = \" \".join(temp)\n",
    "        corpus[i] = temp\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cba7b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = preprocess(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfcba475",
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_grams = {}\n",
    "\n",
    "# loop through each document\n",
    "for i in range(len(corpus)):\n",
    "    \n",
    "    # tokenize the document\n",
    "    tokens = ['<start>'] + nltk.word_tokenize(corpus[i]) \n",
    "    # add start & end tags\n",
    "    tokens = ['<start>'] + tokens + ['<end>']\n",
    "    \n",
    "    # create bi tokens\n",
    "    bi_tokens = bigrams(tokens)    \n",
    "\n",
    "    # add the bi tokens to the full list of bi grams\n",
    "    for bi_token in bi_tokens:\n",
    "        if bi_token not in bi_grams:\n",
    "            bi_grams[bi_token] = 1\n",
    "        else:\n",
    "            bi_grams[bi_token] += 1\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forbidden-suite",
   "metadata": {},
   "source": [
    "### create a dataframe to store the results\n",
    "<b>pre</b> contains the first word, <b>post</b> contains the second work, and <b>count</b> contains the count of the occurences of the two words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cdf838c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pre</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;start&gt;</th>\n",
       "      <td>&lt;start&gt;</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;start&gt;</th>\n",
       "      <td>info</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>info</th>\n",
       "      <td>has</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has</th>\n",
       "      <td>been</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>been</th>\n",
       "      <td>found</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            post  count\n",
       "pre                    \n",
       "<start>  <start>   5000\n",
       "<start>     info      1\n",
       "info         has      1\n",
       "has         been    235\n",
       "been       found      2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dataframe to store the results\n",
    "reults = pd.Series(bi_grams).reset_index().rename(\n",
    "    {'level_0':'pre','level_1':'post',0:'count'},axis=1\n",
    ").set_index('pre')\n",
    "\n",
    "reults.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-stereo",
   "metadata": {},
   "source": [
    "### here, we generate a sentence of up to 10 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b55563be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the last night and the first place in the\n"
     ]
    }
   ],
   "source": [
    "# initialize the sentence with the <start> tag\n",
    "current_word = '<start>'\n",
    "sentence = current_word\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    # when there's no next word prediction, end the loop\n",
    "    if (reults.index==current_word).sum() == 0 or current_word == '<end>':\n",
    "        break\n",
    "        \n",
    "    # when there's only one possible word, set it as the prediction\n",
    "    if (reults.index==initial).sum() == 1:\n",
    "        initial = reults.loc[initial]['post']\n",
    "        \n",
    "    # when there's more than one possible word, take the top 10 words and do a fitness proportionate selection\n",
    "    # Fitness proportionate selection: https://en.wikipedia.org/wiki/Fitness_proportionate_selection\n",
    "    else:\n",
    "        temp = reults.loc[initial].sort_values('count',ascending=False).iloc[:10]\n",
    "        temp['count'] = (temp['count'] / temp['count'].sum()).cumsum()\n",
    "        rnd = np.random.sample()\n",
    "\n",
    "        temp['bullseye'] = temp['count']>rnd\n",
    "\n",
    "        initial = temp[temp['bullseye']]['post'].iloc[0]\n",
    "    \n",
    "    sentence += ' '+initial\n",
    "    \n",
    "print(sentence.replace('<start> ','').replace('<end> ',''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-shore",
   "metadata": {},
   "source": [
    "### results: since we used bi grams, we find that when considering each 2 words separately, they make sense grammatically. However, the full sentence is usually incorrect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-venture",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
